{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The transformative abilities of `sklearn.compose`: a life-saver in disguise?\n",
    "\n",
    "> Note: the initial working title for this talk was \"MLPaaCF: Machine Learning Preprocessing as a Config File,\" which robbed me of the opportunity to make [Transformers](https://en.wikipedia.org/wiki/Transformers_(film_series)) puns several times throughout.\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/stable/) is undoubtedly one of the most popular libraries for machine learning (ML). From the algorithms provided in its core API to other useful capabilities like feature selection, pipelining, and evaluation, scikit-learn has positioned itself as a must-have on the toolbelt of many data folks. In mid-2018, a new submodule for the core scikit-learn library was initiated: `sklearn.compose`. While still relatively slim, this module, when coupled with existing scikit-learn modules like `sklearn.preprocessing`, can be powerful. The goal of this tutorial is to demonstrate how to implement a configuration-based approach to machine learning dataset creation. Specifically, we'll use the [sklearn.compose](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.compose) and [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) modules.\n",
    "\n",
    "The most [recent stable release of scikit-learn](https://scikit-learn.org/dev/versions.html) is version 0.21.3. `sklearn.compose`, by all accounts, seems to have appeared around version 0.20, so the capabilities presented by this section of scikit-learn are relatively new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What dataset will we be using?\n",
    "\n",
    "The [University of California, Irvine (UCI) Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Adult) contains a treasure-trove of datasets for ML work. I chose the [\"Adult\" dataset](https://archive.ics.uci.edu/ml/datasets/Adult), which tasks the analyst with predicting, based off of a variety of inputs, whether an adult makes more or less than $50k per year. This dataset comes with a mixture of real, categorical, and integer features, which ought to make for a much more \"real-world\" dataset-processing example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First, some housekeeping\n",
    "\n",
    "If you haven't already, run `sh setup.sh` from the base directory to:\n",
    "\n",
    "1) Download the \"Adult\" dataset\n",
    "\n",
    "2) Set up a virtual environment for dependency management\n",
    "\n",
    "3) Start the Jupyter Notebook server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Getting started with the actual exercise\n",
    "\n",
    "First, we'll load the adult dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gathered from the adult.names file and posted here for your convenience\n",
    "cols = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per',\n",
    "    'native-country',\n",
    "    'makes_gt_50k'\n",
    "]\n",
    "\n",
    "df = pd.read_csv('data/adult.data', names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at some metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (32561, 15)\n",
      "Data sample:\n",
      "   age          workclass  fnlwgt   education  education-num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital-status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per  native-country makes_gt_50k  \n",
      "0          2174             0         40   United-States        <=50K  \n",
      "1             0             0         13   United-States        <=50K  \n",
      "2             0             0         40   United-States        <=50K  \n",
      "3             0             0         40   United-States        <=50K  \n",
      "4             0             0         40            Cuba        <=50K  \n",
      "Data types:\n",
      "age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per          int64\n",
      "native-country    object\n",
      "makes_gt_50k      object\n",
      "dtype: object\n",
      "Number of unique values by field, for non-numeric features:\n",
      "workclass          9\n",
      "education         16\n",
      "marital-status     7\n",
      "occupation        15\n",
      "relationship       6\n",
      "race               5\n",
      "sex                2\n",
      "native-country    42\n",
      "makes_gt_50k       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of dataset: {df.shape}')\n",
    "print(f'Data sample:\\n{df.head()}')\n",
    "print(f'Data types:\\n{df.dtypes}')\n",
    "print(f'Number of unique values by field, for non-numeric features:\\n{df.select_dtypes(include=[\"object\"]).nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is quite a diversity of fields in this dataset. We have a mixture of continuous (`age`, `capital-gain`, `hours-per`, etc.) and categorical (`workclass`, `education`, etc.) features.\n",
    "\n",
    "Now, a logical next step in the process of building a predictive model would be to perform some exploratory data analysis on each of the potential input features. **For the sake of this exercise**, let's assume we've done that and proceed straight to feature-engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Ah, feature engineering - as the old (well, as old as the term \"data scientist\") adage goes, about 80% of your time will be spent pulling together features for whatever model you're building. Now, the vast majority of this time is spent working with stakeholders, thinking about the domain, and trying to come up with the most relevant predictors for whatever predictive task you're after.\n",
    "\n",
    "However, once you've got all of your main features pulled together, oftentimes that's just the first step (albeit a very large one): you'll likely need to preprocess a lot of the fields in order to make your data play nicely with whatever ML algorithm software you're trying to use.\n",
    "\n",
    "For example, most of the algorithms in Python's main ML libraries don't natively support mixed types in input datasets. That is to say, instead of feeding a vector for `sex` like `['male', 'female', 'male', 'female']` as an input feature, we will instead need to encode this field in a numerical fashion. By far the most common approach for encoding categorical vectors is called \"one-hot encoding.\" Below, I'll show a few (of many) examples of how one-hot encoding can be accomplished in Python.\n",
    "\n",
    "> Note: oftentimes, preprocessing will be applied across the entire dataset - not just for categorical features. For the sake of brevity, I'll only demonstrate the one-hot-encoding approach and leave it up to you to incorporate more sophisticated encoding strategies for features of other types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `pandas.get_dummies`\n",
    "\n",
    "The data-manipulation library `pandas` has a function called `get_dummies`, which creates [\"dummy\" variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)), given some input. Here's an example of how we might encode `sex` using `pandas.get_dummies`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original column:\n",
      "0       Male\n",
      "1       Male\n",
      "2       Male\n",
      "3       Male\n",
      "4     Female\n",
      "5     Female\n",
      "6     Female\n",
      "7       Male\n",
      "8     Female\n",
      "9       Male\n",
      "Name: sex, dtype: object\n",
      "That same column, one-hot-encoded:\n",
      "   sex_ Female  sex_ Male\n",
      "0            0          1\n",
      "1            0          1\n",
      "2            0          1\n",
      "3            0          1\n",
      "4            1          0\n",
      "5            1          0\n",
      "6            1          0\n",
      "7            0          1\n",
      "8            1          0\n",
      "9            0          1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original column:\\n{df['sex'].head(10)}\")\n",
    "print(f\"That same column, one-hot-encoded:\\n{pd.get_dummies(df['sex'], prefix='sex').head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach would be to use [`sklearn.preprocessing.OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That same column, one-hot-encoded:\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Note: when using models prone to perfect collinearity, you'll want to set `drop=True`\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "print(f\"That same column, one-hot-encoded:\\n{enc.fit_transform(df['sex'].values.reshape(-1, 1))[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, both of these approaches are perfectly fine ways of performing one-hot encoding. However, the latter approach will play very nicely with the rest of the `sklearn.compose` module, which I'm here to demonstrate. Technically, `pd.get_dummies` could work as well, but it would take a bit more work, and the main benefit of the second approach is staying within the `scikit-learn` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how does `sklearn.compose` help with all of this preprocessing?\n",
    "\n",
    "If you look at the [main page for `sklearn.compose`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.compose), you'll notice how few functions/classes exist within that submodule. We're most concerned with `ColumnTransformer` and `make_column_transformer`. From the `ColumnTransformer` description:\n",
    "\n",
    "> Applies transformers to columns of an array or pandas DataFrame.\n",
    "This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.\n",
    "\n",
    "The `make_column_transformer` is simply shorthand for `ColumnTransformer`, and doesn't support as many options as its namesake, so for this exercise we'll concern ourselves primarily with the latter.\n",
    "\n",
    "Effectively, the code provided through the `compose` submodule will allow us to very easily construct analytic, ready-to-be-modeled-off-of datasets, using pre-defined encoding patterns.\n",
    "\n",
    "### Enough talk â€“ how does a `ColumnTransformer` work?\n",
    "\n",
    "Let's go through the one-hot encoding example from above, using a `ColumnTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "col = 'sex'\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "trans = ColumnTransformer([(col, enc, [col])])\n",
    "trans.fit_transform(df)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we've successfully use a `ColumnTransformer` and the typical scikit-learn `fit` and `transform` patterns to one-hot-encode the `sex` column, just like we did above.\n",
    "\n",
    "<img src=\"imgs/ytho.jpg\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of the `ColumnTransformer` is seen when you're dealing with trying to abstract more complex encoding pipelines across a \"wide\" dataset. Sure, properly encoding `sex` by itself is a trivial task. But what if you have to apply *d*-number of encoding strategies across *n*-number of columns, and want a consistent and \"summarized\" way of doing so? Allow me to demonstrate.\n",
    "\n",
    "We're effectively going to treat the encoding/preprocessing step as a configuration file problem. First, we'll select a few columns (of whatever type), and specify what type of feature the column represents.\n",
    "\n",
    "> Note: you could also get at the type of feature by relying on pandas' default data-type parsing when the file is initially read, i.e. looking at `df.dtypes`. What is shown in this tutorial is a more explicit approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0., ...,  0.,  1., 39.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 50.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 38.],\n",
       "       ...,\n",
       "       [ 1.,  0.,  0., ...,  0.,  1., 58.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 22.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  1., 52.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategies = [\n",
    "    {\n",
    "        'col': 'sex',\n",
    "        'kind': 'categorical'\n",
    "    },\n",
    "    {\n",
    "        'col':  'race',\n",
    "        'kind': 'categorical'\n",
    "    },\n",
    "    {\n",
    "        'col': 'age',\n",
    "        'kind': 'continuous'\n",
    "    }\n",
    "]\n",
    "\n",
    "transformers = []\n",
    "for s in strategies:\n",
    "    col, kind = s['col'], s['kind']\n",
    "    \n",
    "    # An opinionated encoding mechanism\n",
    "    if kind == 'categorical':\n",
    "        transformer = OneHotEncoder(sparse=False)\n",
    "    elif kind == 'continuous':\n",
    "        # Default to not applying any preprocessing to continuous features\n",
    "        transformer = 'passthrough'\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    result = (col, transformer, [col])\n",
    "    transformers.append(result)\n",
    "\n",
    "master_trans = ColumnTransformer(transformers)\n",
    "master_trans.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced topics\n",
    "\n",
    "#### Getting names of encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class SimpleImputerWithFeatureNames(SimpleImputer):\n",
    "    def fit(self, X, y):\n",
    "        self.feature_names = list(X)\n",
    "        super().fit(X)\n",
    "        return self\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "class PassthroughEncoder(BaseEstimator, TransformerMixin):\n",
    "    # X is a pandas.DataFrame or pandas.Series, in this case\n",
    "    def fit(self, X, y):\n",
    "        self.feature_names = list(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_handler(\n",
    "    kind,\n",
    "    fill_value,\n",
    "    ignore_fill_value,\n",
    "    handle_unknown):\n",
    "\n",
    "    ''' Generic feature strategy handler '''\n",
    "    if kind == 'categorical':\n",
    "        return OneHotEncoder(sparse=False, handle_unknown=handle_unknown)\n",
    "    elif kind == 'continuous':\n",
    "        if ignore_fill_value:\n",
    "            return PassthroughEncoder()\n",
    "        else:\n",
    "            # These are the four strategies supported by SimpleImputer currently\n",
    "            if fill_value in {'mean', 'median', 'most_frequent', 'constant'}:\n",
    "                params = {'strategy': fill_value, 'fill_value': None}\n",
    "            else:\n",
    "                params = {'strategy': 'constant', 'fill_value': fill_value}\n",
    "            return SimpleImputerWithFeatureNames(**params)\n",
    "    else:\n",
    "        raise ValueError('Kind \"{}\" invalid. Try \"continuous\" or \"categorical\"'.format(kind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(strategies, ignore_fill_value, handle_unknown):\n",
    "    ''' Take entire `strategies` (see model_config.py) and create master transformer '''\n",
    "    transformers = []\n",
    "    for s in strategies:\n",
    "        name, kind, fill_value = s['name'], s['kind'], s['fill_value']\n",
    "        transformer = strategy_handler(\n",
    "            kind,\n",
    "            fill_value,\n",
    "            ignore_fill_value,\n",
    "            handle_unknown\n",
    "        )\n",
    "        result = (name, transformer, [name])\n",
    "        transformers.append(result)\n",
    "    return ColumnTransformer(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [\n",
    "    {\n",
    "        'name': 'age',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'workclass',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'fnlwgt',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "#     {\n",
    "#         'name': 'education',\n",
    "#         'kind': 'categorical',\n",
    "#         'fill_value': np.nan\n",
    "#     },\n",
    "    {\n",
    "        'name': 'education-num',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'marital-status',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'occupation',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'relationship',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'race',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'sex',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'capital-gain',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'capital-loss',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'hours-per',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'native-country',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "]\n",
    "\n",
    "df['make_gt_50k'] = np.where(df['makes_gt_50k'] == ' >50K', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = build_transformer(strategies, True, False)\n",
    "transformer.fit_transform(df)\n",
    "\n",
    "# TODO: show original versus monitoring transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leave-one-out, rule-of-*n*\n",
    "PassthroughTransformer to get at `get_feature_names`\n",
    "Fill values\n",
    "Accepts any sort-of scikit Estimator, with fit and fit_transform stuff"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
