{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The transformative abilities of `sklearn.compose`: a life-saver in disguise?\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/stable/) is undoubtedly one of the most popular libraries for machine learning (ML). From the algorithms provided in its core API to other useful capabilities like feature selection, pipelining, and evaluation, scikit-learn has positioned itself as a must-have on the toolbelt of many data folks. In mid-2018, a new submodule for the core scikit-learn library was initiated: `sklearn.compose`. While still relatively slim, this module, when coupled with existing scikit-learn modules like `sklearn.preprocessing`, can be powerful. The goal of this tutorial is to demonstrate how to implement a configuration-based approach to machine learning dataset creation. Specifically, we'll use the [sklearn.compose](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.compose) and [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) modules.\n",
    "\n",
    "The most [recent stable release of scikit-learn](https://scikit-learn.org/dev/versions.html) is version 0.21.3. `sklearn.compose`, by all accounts, seems to have appeared around version 0.20, so the capabilities presented by this section of scikit-learn are relatively new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What dataset will we be using?\n",
    "\n",
    "The [University of California, Irvine (UCI) Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Adult) contains a treasure-trove of datasets for ML work. I chose the [\"Adult\" dataset](https://archive.ics.uci.edu/ml/datasets/Adult), which tasks the analyst with predicting, based off of a variety of inputs, whether an adult makes more or less than $50k per year. This dataset comes with a mixture of real, categorical, and integer features, which ought to make for a much more \"real-world\" dataset-processing example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First, some housekeeping\n",
    "\n",
    "If you haven't already, run `sh setup.sh` from the base directory to:\n",
    "\n",
    "1) Download the \"Adult\" dataset\n",
    "\n",
    "2) Set up a virtual environment for dependency management\n",
    "\n",
    "3) Start the Jupyter Notebook server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Getting started with the actual exercise\n",
    "\n",
    "First, we'll load the adult dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gathered from the adult.names file and posted here for your convenience\n",
    "cols = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per',\n",
    "    'native-country',\n",
    "    'makes_gt_50k'\n",
    "]\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at some metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (32561, 15)\n",
      "Data sample:\n",
      "   age          workclass  fnlwgt   education  education-num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital-status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per  native-country makes_gt_50k  \n",
      "0          2174             0         40   United-States        <=50K  \n",
      "1             0             0         13   United-States        <=50K  \n",
      "2             0             0         40   United-States        <=50K  \n",
      "3             0             0         40   United-States        <=50K  \n",
      "4             0             0         40            Cuba        <=50K  \n",
      "Data types:\n",
      "age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per          int64\n",
      "native-country    object\n",
      "makes_gt_50k      object\n",
      "dtype: object\n",
      "Number of unique values by field, for non-numeric features:\n",
      "workclass          9\n",
      "education         16\n",
      "marital-status     7\n",
      "occupation        15\n",
      "relationship       6\n",
      "race               5\n",
      "sex                2\n",
      "native-country    42\n",
      "makes_gt_50k       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of dataset: {df.shape}')\n",
    "print(f'Data sample:\\n{df.head()}')\n",
    "print(f'Data types:\\n{df.dtypes}')\n",
    "print(f'Number of unique values by field, for non-numeric features:\\n{df.select_dtypes(include=[\"object\"]).nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is quite a diversity of fields in this dataset. We have a mixture of continuous (`age`, `capital-gain`, `hours-per`, etc.) and categorical (`workclass`, `education`, etc.) features.\n",
    "\n",
    "Now, a logical next step in the process of building a predictive model would be to perform some exploratory data analysis on each of the potential input features. **For the sake of this exercise**, let's assume we've done that and proceed straight to feature-engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Ah, feature engineering - as the old (well, as old as the term \"data scientist\") adage goes, about 80% of your time will be spent pulling together features for whatever model you're building. Now, the vast majority of this time is spent working with stakeholders, thinking about the domain, and trying to come up with the most relevant predictors for whatever predictive task you're after.\n",
    "\n",
    "However, once you've got all of your main features pulled together, oftentimes that's just the first step (albeit a very large one): you'll likely need to preprocess a lot of the fields in order to make your data play nicely with whatever ML algorithm software you're trying to use.\n",
    "\n",
    "For example, most of the algorithms in Python's main ML libraries don't natively support mixed types in input datasets. That is to say, instead of feeding a vector for `sex` like `['male', 'female', 'male', 'female']` as an input feature, we will instead need to encode this field in a numerical fashion. By far the most common approach for encoding categorical vectors is called \"one-hot encoding.\" Below, I'll show a few (of many) examples of how one-hot encoding can be accomplished in Python.\n",
    "\n",
    "> Note: oftentimes, preprocessing will be applied across the entire dataset - not just for categorical features. For the sake of brevity, I'll only demonstrate the one-hot-encoding approach and leave it up to you to incorporate more sophisticated encoding strategies for features of other types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `pandas.get_dummies`\n",
    "\n",
    "The data-manipulation library `pandas` has a function called `get_dummies`, which creates [\"dummy\" variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)), given some input. Here's an example of how we might encode `sex` using `pandas.get_dummies`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original column:\n",
      "0       Male\n",
      "1       Male\n",
      "2       Male\n",
      "3       Male\n",
      "4     Female\n",
      "5     Female\n",
      "6     Female\n",
      "7       Male\n",
      "8     Female\n",
      "9       Male\n",
      "Name: sex, dtype: object\n",
      "That same column, one-hot-encoded:\n",
      "   sex_ Female  sex_ Male\n",
      "0            0          1\n",
      "1            0          1\n",
      "2            0          1\n",
      "3            0          1\n",
      "4            1          0\n",
      "5            1          0\n",
      "6            1          0\n",
      "7            0          1\n",
      "8            1          0\n",
      "9            0          1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original column:\\n{df['sex'].head(10)}\")\n",
    "print(f\"That same column, one-hot-encoded:\\n{pd.get_dummies(df['sex'], prefix='sex').head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach would be to use [`sklearn.preprocessing.OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That same column, one-hot-encoded:\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Note: when using models prone to perfect collinearity, you'll want to set `drop=True`\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "print(f\"That same column, one-hot-encoded:\\n{enc.fit_transform(df['sex'].values.reshape(-1, 1))[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, both of these approaches are perfectly fine ways of performing one-hot encoding. However, the latter approach will play very nicely with the rest of the `sklearn.compose` module, which I'm here to demonstrate. Technically, `pd.get_dummies` could work as well, but it would take a bit more work, and the main benefit of the second approach is staying within the `scikit-learn` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how does `sklearn.compose` help with all of this preprocessing?\n",
    "\n",
    "If you look at the [main page for `sklearn.compose`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.compose), you'll notice how few functions/classes exist within that submodule. We're most concerned with `ColumnTransformer` and `make_column_transformer`. From the `ColumnTransformer` description:\n",
    "\n",
    "> Applies transformers to columns of an array or pandas DataFrame.\n",
    "This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.\n",
    "\n",
    "The `make_column_transformer` is simply shorthand for `ColumnTransformer`, and doesn't support as many options as its namesake, so for this exercise we'll concern ourselves primarily with the latter.\n",
    "\n",
    "Effectively, the code provided through the `compose` submodule will allow us to very easily construct analytic, ready-to-be-modeled-off-of datasets, using pre-defined encoding patterns.\n",
    "\n",
    "### Enough talk â€“ how does a `ColumnTransformer` work?\n",
    "\n",
    "Let's go through the one-hot encoding example from above, using a `ColumnTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "col = 'sex'\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "trans = ColumnTransformer([(col, enc, [col])])\n",
    "trans.fit_transform(df)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we've successfully use a `ColumnTransformer` and the typical scikit-learn `fit` and `transform` patterns to one-hot-encode the `sex` column, just like we did above.\n",
    "\n",
    "<img src=\"imgs/ytho.jpg\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of the `ColumnTransformer` is seen when you're dealing with trying to abstract more complex encoding pipelines across a \"wide\" dataset. Sure, properly encoding `sex` by itself is a trivial task. But what if you have to apply *d*-number of encoding strategies across *n*-number of columns, and want a consistent and \"summarized\" way of doing so? Allow me to demonstrate.\n",
    "\n",
    "We're effectively going to treat the encoding/preprocessing step as a configuration file problem. First, we'll select a few columns (of whatever type), and specify what type of feature the column represents.\n",
    "\n",
    "> Note: you could also get at the type of feature by relying on pandas' default data-type parsing when the file is initially read, i.e. looking at `df.dtypes`. What is shown in this tutorial is a more explicit approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0., ...,  0.,  1., 39.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 50.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 38.],\n",
       "       ...,\n",
       "       [ 1.,  0.,  0., ...,  0.,  1., 58.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 22.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  1., 52.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategies = [\n",
    "    {\n",
    "        'col': 'sex',\n",
    "        'kind': 'categorical'\n",
    "    },\n",
    "    {\n",
    "        'col':  'race',\n",
    "        'kind': 'categorical'\n",
    "    },\n",
    "    {\n",
    "        'col': 'age',\n",
    "        'kind': 'continuous'\n",
    "    }\n",
    "]\n",
    "\n",
    "transformers = []\n",
    "for s in strategies:\n",
    "    col, kind = s['col'], s['kind']\n",
    "    \n",
    "    # An opinionated encoding mechanism\n",
    "    if kind == 'categorical':\n",
    "        transformer = OneHotEncoder(sparse=False)\n",
    "    elif kind == 'continuous':\n",
    "        # Default to not applying any preprocessing to continuous features\n",
    "        transformer = 'passthrough'\n",
    "    else:\n",
    "        # Add support at some point for other data types\n",
    "        pass\n",
    "    \n",
    "    result = (col, transformer, [col])\n",
    "    transformers.append(result)\n",
    "\n",
    "master_trans = ColumnTransformer(transformers)\n",
    "master_trans.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional topics\n",
    "\n",
    "In the example above, we went from a raw, unprocessed dataset to something ready for further analysis/modeling, all in very few lines of code thanks to our dataset-as-a-configuration-file approach.\n",
    "\n",
    "But there is still more to touch on here. Below I'll detail some additional topics related to `sklearn.compose`.\n",
    "\n",
    "#### Getting names of encoded features\n",
    "\n",
    "All we see above is a NumPy matrix containing a bunch of numbers. What if we wanted to more easily inspect or share this dataset? One of the first things we may want to know is the human-readable name for each column.\n",
    "\n",
    "In comes the `get_feature_names` method off of our `ColumnTransformer` object. Let's see what results we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "get_feature_names is not yet supported when using a 'passthrough' transformer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d91e9be7d35f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaster_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 raise NotImplementedError(\n\u001b[0;32m--> 334\u001b[0;31m                     \u001b[0;34m\"get_feature_names is not yet supported when using \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                     \"a 'passthrough' transformer.\")\n\u001b[1;32m    336\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_feature_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: get_feature_names is not yet supported when using a 'passthrough' transformer."
     ]
    }
   ],
   "source": [
    "master_trans.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error is to be expected (at least at this point in `sklearn.compose`'s development cycle)! Since we've used the `'passthrough'` option for some of our features, we don't have the ability to get the name of the feature than was originally transformed. But, just because that capability isn't yet enabled for the \"passthrough\" transformation doesn't mean we can't write that functionality ourselves! ðŸ˜ˆ\n",
    "\n",
    "> Note: the `sklearn.preprocessing.SimpleImputer` class also doesn't have a `get_feature_names` method - so we'll work around that below as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "# Inherit sklearn's BaseEstimator and TransformerMixin classes to make these new\n",
    "# classes play nicely with the rest of the `sklearn.compose` functionality we're using\n",
    "class PassthroughEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        self.feature_names = list(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "class SimpleImputerWithFeatureNames(SimpleImputer):\n",
    "    # X is a pandas.DataFrame or pandas.Series, in this case\n",
    "    def fit(self, X, y = None):\n",
    "        self.feature_names = list(X)\n",
    "        # Execute parent class method - the nuts and bolts of this child object\n",
    "        super().fit(X)\n",
    "        return self\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0., ...,  0.,  1., 39.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 50.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 38.],\n",
       "       ...,\n",
       "       [ 1.,  0.,  0., ...,  0.,  1., 58.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 22.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  1., 52.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers = []\n",
    "for s in strategies:\n",
    "    col, kind = s['col'], s['kind']\n",
    "    \n",
    "    # An opinionated encoding mechanism\n",
    "    if kind == 'categorical':\n",
    "        transformer = OneHotEncoder(sparse=False)\n",
    "    elif kind == 'continuous':\n",
    "        # Default to not applying any preprocessing to continuous features\n",
    "        transformer = PassthroughEncoder()\n",
    "    else:\n",
    "        # Add support at some point for other data types\n",
    "        pass\n",
    "    \n",
    "    result = (col, transformer, [col])\n",
    "    transformers.append(result)\n",
    "\n",
    "trans = ColumnTransformer(transformers)\n",
    "trans.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the feature names for this `ColumnTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sex__x0_ Female',\n",
       " 'sex__x0_ Male',\n",
       " 'race__x0_ Amer-Indian-Eskimo',\n",
       " 'race__x0_ Asian-Pac-Islander',\n",
       " 'race__x0_ Black',\n",
       " 'race__x0_ Other',\n",
       " 'race__x0_ White',\n",
       " 'age__age']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_handler(\n",
    "    kind,\n",
    "    fill_value,\n",
    "    ignore_fill_value,\n",
    "    handle_unknown):\n",
    "\n",
    "    ''' Generic feature strategy handler '''\n",
    "    if kind == 'categorical':\n",
    "        return OneHotEncoder(sparse=False, handle_unknown=handle_unknown)\n",
    "    elif kind == 'continuous':\n",
    "        if ignore_fill_value:\n",
    "            return PassthroughEncoder()\n",
    "        else:\n",
    "            # These are the four strategies supported by SimpleImputer currently\n",
    "            if fill_value in {'mean', 'median', 'most_frequent', 'constant'}:\n",
    "                params = {'strategy': fill_value, 'fill_value': None}\n",
    "            else:\n",
    "                params = {'strategy': 'constant', 'fill_value': fill_value}\n",
    "            return SimpleImputerWithFeatureNames(**params)\n",
    "    else:\n",
    "        raise ValueError('Kind \"{}\" invalid. Try \"continuous\" or \"categorical\"'.format(kind))\n",
    "        \n",
    "def build_transformer(strategies, ignore_fill_value, handle_unknown):\n",
    "    ''' Take entire `strategies` (see model_config.py) and create master transformer '''\n",
    "    transformers = []\n",
    "    for s in strategies:\n",
    "        name, kind, fill_value = s['name'], s['kind'], s['fill_value']\n",
    "        transformer = strategy_handler(\n",
    "            kind,\n",
    "            fill_value,\n",
    "            ignore_fill_value,\n",
    "            handle_unknown\n",
    "        )\n",
    "        result = (name, transformer, [name])\n",
    "        transformers.append(result)\n",
    "    return ColumnTransformer(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor(df, transformer):\n",
    "    '''\n",
    "    Iterate through each transformer in an\n",
    "    already-fitted ColumnTransformer to check\n",
    "    whether new data raises any Exceptions (intended or otherwise)\n",
    "    '''\n",
    "\n",
    "    if not hasattr(transformer, 'transformers_'):\n",
    "        print('Transformer must be fitted first')\n",
    "    else:\n",
    "        # Select only those transformers that inherit from the BaseEstimator class\n",
    "        transformers = [\n",
    "            (t_inst, t_col)\n",
    "            for _, t_inst, t_col in transformer.transformers_\n",
    "            if isinstance(t_inst, BaseEstimator)\n",
    "        ]\n",
    "\n",
    "        for t_inst, t_col in transformers:\n",
    "            try:\n",
    "                t_inst.transform(df[t_col])\n",
    "            except Exception as e:\n",
    "                e = str(e)\n",
    "                if 'unknown categories' in e:\n",
    "                    print(f'{e} ... found in column \"{t_col}\"')\n",
    "                else:\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [\n",
    "    {\n",
    "        'name': 'age',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'workclass',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'fnlwgt',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "#     {\n",
    "#         'name': 'education',\n",
    "#         'kind': 'categorical',\n",
    "#         'fill_value': np.nan\n",
    "#     },\n",
    "    {\n",
    "        'name': 'education-num',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'marital-status',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'occupation',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'relationship',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'race',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'sex',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'capital-gain',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'capital-loss',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'hours-per',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'name': 'native-country',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "]\n",
    "\n",
    "df['make_gt_50k'] = np.where(df['makes_gt_50k'] == ' >50K', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = build_transformer(strategies, True, 'ignore')\n",
    "transformer.fit_transform(df)\n",
    "\n",
    "# TODO: show original versus monitoring transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leave-one-out, rule-of-*n*\n",
    "PassthroughTransformer to get at `get_feature_names`\n",
    "Fill values\n",
    "Accepts any sort-of scikit Estimator, with fit and fit_transform stuff"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
