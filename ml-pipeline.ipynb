{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The transformative abilities of `sklearn.compose`: a life-saver in disguise?\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/stable/) is undoubtedly one of the most popular libraries for machine learning (ML). From the algorithms provided in its core API to other useful capabilities like feature selection, pipelining, and evaluation, scikit-learn has positioned itself as a must-have on the toolbelt of many data folks. In mid-2018, a new submodule for the core scikit-learn library was initiated: `sklearn.compose`. While still relatively slim, this module, when coupled with existing scikit-learn modules like `sklearn.preprocessing`, can be powerful. The goal of this tutorial is to demonstrate how to implement a configuration-based approach to machine learning dataset creation. Specifically, we'll use the [sklearn.compose](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.compose) and [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) modules.\n",
    "\n",
    "The most [recent stable release of scikit-learn](https://scikit-learn.org/dev/versions.html) is version 0.21.3. `sklearn.compose`, by all accounts, seems to have appeared around version 0.20, so the capabilities presented by this section of scikit-learn are relatively new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What dataset will we be using?\n",
    "\n",
    "The [University of California, Irvine (UCI) Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Adult) contains a treasure-trove of datasets for ML work. I chose the [\"Adult\" dataset](https://archive.ics.uci.edu/ml/datasets/Adult), which tasks the analyst with predicting, based off of a variety of inputs, whether an adult makes more or less than $50k per year. This dataset comes with a mixture of real, categorical, and integer features, which ought to make for a much more \"real-world\" dataset-processing example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First, some housekeeping\n",
    "\n",
    "If you haven't already, run `sh setup.sh` from the base directory to:\n",
    "\n",
    "1) Set up a virtual environment for dependency management\n",
    "\n",
    "2) Start the Jupyter Notebook server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Getting started with the actual exercise\n",
    "\n",
    "First, we'll load the adult dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gathered from the adult.names file and posted here for your convenience\n",
    "cols = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per',\n",
    "    'native-country',\n",
    "    'makes_gt_50k'\n",
    "]\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at some metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (32561, 15)\n",
      "Data sample:\n",
      "   age          workclass  fnlwgt   education  education-num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital-status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per  native-country makes_gt_50k  \n",
      "0          2174             0         40   United-States        <=50K  \n",
      "1             0             0         13   United-States        <=50K  \n",
      "2             0             0         40   United-States        <=50K  \n",
      "3             0             0         40   United-States        <=50K  \n",
      "4             0             0         40            Cuba        <=50K  \n",
      "Data types:\n",
      "age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per          int64\n",
      "native-country    object\n",
      "makes_gt_50k      object\n",
      "dtype: object\n",
      "Number of unique values by field, for non-numeric features:\n",
      "workclass          9\n",
      "education         16\n",
      "marital-status     7\n",
      "occupation        15\n",
      "relationship       6\n",
      "race               5\n",
      "sex                2\n",
      "native-country    42\n",
      "makes_gt_50k       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of dataset: {df.shape}')\n",
    "print(f'Data sample:\\n{df.head()}')\n",
    "print(f'Data types:\\n{df.dtypes}')\n",
    "print(f'Number of unique values by field, for non-numeric features:\\n{df.select_dtypes(include=[\"object\"]).nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is quite a diversity of fields in this dataset. We have a mixture of continuous (`age`, `capital-gain`, `hours-per`, etc.) and categorical (`workclass`, `education`, etc.) features.\n",
    "\n",
    "Now, a logical next step in the process of building a predictive model would be to perform some exploratory data analysis on each of the potential input features. **For the sake of this exercise**, let's assume we've done that and proceed straight to feature-engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Ah, feature engineering - as the old (well, as old as the term \"data scientist\") adage goes, about 80% of your time will be spent pulling together features for whatever model you're building. Now, the vast majority of this time is spent working with stakeholders, thinking about the domain, and trying to come up with the most relevant predictors for whatever predictive task you're after.\n",
    "\n",
    "However, once you've got all of your main features pulled together, oftentimes that's just the first step (albeit a very large one): you'll likely need to preprocess a lot of the fields in order to make your data play nicely with whatever ML algorithm software you're trying to use.\n",
    "\n",
    "For example, most of the algorithms in Python's main ML libraries don't natively support mixed types in input datasets. That is to say, instead of feeding a vector for `sex` like `['male', 'female', 'male', 'female']` as an input feature, we will instead need to encode this field in a numerical fashion. By far the most common approach for encoding categorical vectors is called \"one-hot encoding.\" Below, I'll show a few (of many) examples of how one-hot encoding can be accomplished in Python.\n",
    "\n",
    "> Note: oftentimes, preprocessing will be applied across the entire dataset - not just for categorical features. For the sake of brevity, I'll only demonstrate the one-hot-encoding approach and leave it up to you to incorporate more sophisticated encoding strategies for features of other types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `pandas.get_dummies`\n",
    "\n",
    "The data-manipulation library `pandas` has a function called `get_dummies`, which creates [\"dummy\" variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)), given some input. Here's an example of how we might encode `sex` using `pandas.get_dummies`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original column:\n",
      "0       Male\n",
      "1       Male\n",
      "2       Male\n",
      "3       Male\n",
      "4     Female\n",
      "5     Female\n",
      "6     Female\n",
      "7       Male\n",
      "8     Female\n",
      "9       Male\n",
      "Name: sex, dtype: object\n",
      "That same column, one-hot-encoded:\n",
      "   sex_ Female  sex_ Male\n",
      "0            0          1\n",
      "1            0          1\n",
      "2            0          1\n",
      "3            0          1\n",
      "4            1          0\n",
      "5            1          0\n",
      "6            1          0\n",
      "7            0          1\n",
      "8            1          0\n",
      "9            0          1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original column:\\n{df['sex'].head(10)}\")\n",
    "print(f\"That same column, one-hot-encoded:\\n{pd.get_dummies(df['sex'], prefix='sex').head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach would be to use [`sklearn.preprocessing.OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That same column, one-hot-encoded:\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Note: when using models prone to perfect collinearity, you'll want to set `drop=True`\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "print(f\"That same column, one-hot-encoded:\\n{enc.fit_transform(df['sex'].values.reshape(-1, 1))[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, both of these approaches are perfectly fine ways of performing one-hot encoding. However, the latter approach will play very nicely with the rest of the `sklearn.compose` module, which I'm here to demonstrate. Technically, `pd.get_dummies` could work as well, but it would take a bit more work, and the main benefit of the second approach is staying within the `scikit-learn` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how does `sklearn.compose` help with all of this preprocessing?\n",
    "\n",
    "If you look at the [main page for `sklearn.compose`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.compose), you'll notice how few functions/classes exist within that submodule. We're most concerned with `ColumnTransformer` and `make_column_transformer`. From the `ColumnTransformer` description:\n",
    "\n",
    "> Applies transformers to columns of an array or pandas DataFrame.\n",
    "This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.\n",
    "\n",
    "The `make_column_transformer` is simply shorthand for `ColumnTransformer`, and doesn't support as many options as its namesake, so for this exercise we'll concern ourselves primarily with the latter.\n",
    "\n",
    "Effectively, the code provided through the `compose` submodule will allow us to very easily construct analytic, ready-to-be-modeled-off-of datasets, using pre-defined encoding patterns.\n",
    "\n",
    "### Enough talk â€“ how does a `ColumnTransformer` work?\n",
    "\n",
    "Let's go through the one-hot encoding example from above, using a `ColumnTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "col = 'sex'\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "trans = ColumnTransformer([(col, enc, [col])])\n",
    "trans.fit_transform(df)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we've successfully use a `ColumnTransformer` and the typical scikit-learn `fit` and `transform` patterns to one-hot-encode the `sex` column, just like we did above.\n",
    "\n",
    "<img src=\"imgs/ytho.jpg\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of the `ColumnTransformer` is seen when you're dealing with trying to abstract more complex encoding pipelines across a \"wide\" dataset. Sure, properly encoding `sex` by itself is a trivial task. But what if you have to apply *d*-number of encoding strategies across *n*-number of columns, and want a consistent and \"summarized\" way of doing so? Allow me to demonstrate.\n",
    "\n",
    "We're effectively going to treat the encoding/preprocessing step as a configuration file problem. First, we'll select a few columns (of whatever type), and specify what type of feature the column represents.\n",
    "\n",
    "> Note: you could also get at the type of feature by relying on pandas' default data-type parsing when the file is initially read, i.e. looking at `df.dtypes`. What is shown in this tutorial is a more explicit approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0., ...,  0.,  1., 39.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 50.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 38.],\n",
       "       ...,\n",
       "       [ 1.,  0.,  0., ...,  0.,  1., 58.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 22.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  1., 52.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategies = [\n",
    "    {\n",
    "        'col': 'sex',\n",
    "        'kind': 'categorical'\n",
    "    },\n",
    "    {\n",
    "        'col':  'race',\n",
    "        'kind': 'categorical'\n",
    "    },\n",
    "    {\n",
    "        'col': 'age',\n",
    "        'kind': 'continuous'\n",
    "    }\n",
    "]\n",
    "\n",
    "transformers = []\n",
    "for s in strategies:\n",
    "    col, kind = s['col'], s['kind']\n",
    "    \n",
    "    # An opinionated encoding mechanism\n",
    "    if kind == 'categorical':\n",
    "        transformer = OneHotEncoder(sparse=False)\n",
    "    elif kind == 'continuous':\n",
    "        # Default to not applying any preprocessing to continuous features\n",
    "        transformer = 'passthrough'\n",
    "    else:\n",
    "        # Add support at some point for other data types\n",
    "        pass\n",
    "    \n",
    "    result = (col, transformer, [col])\n",
    "    transformers.append(result)\n",
    "\n",
    "master_trans = ColumnTransformer(transformers)\n",
    "master_trans.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional topics\n",
    "\n",
    "In the example above, we went from a raw, unprocessed dataset to something ready for further analysis/modeling, all in very few lines of code thanks to our dataset-as-a-configuration-file approach.\n",
    "\n",
    "But there is still more to touch on here. Below I'll detail some additional topics related to `sklearn.compose`.\n",
    "\n",
    "#### Getting names of encoded features\n",
    "\n",
    "All we see above is a NumPy matrix containing a bunch of numbers. What if we wanted to more easily inspect or share this dataset? One of the first things we may want to know is the human-readable name for each column.\n",
    "\n",
    "In comes the `get_feature_names` method off of our `ColumnTransformer` object. Let's see what results we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "get_feature_names is not yet supported when using a 'passthrough' transformer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d91e9be7d35f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaster_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 raise NotImplementedError(\n\u001b[0;32m--> 334\u001b[0;31m                     \u001b[0;34m\"get_feature_names is not yet supported when using \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                     \"a 'passthrough' transformer.\")\n\u001b[1;32m    336\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_feature_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: get_feature_names is not yet supported when using a 'passthrough' transformer."
     ]
    }
   ],
   "source": [
    "master_trans.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error is to be expected (at least at this point in `sklearn.compose`'s development cycle)! Since we've used the `'passthrough'` option for some of our features, we don't have the ability to get the name of the feature than was originally transformed. But, just because that capability isn't yet enabled for the \"passthrough\" transformation doesn't mean we can't write that functionality ourselves! ðŸ˜ˆ\n",
    "\n",
    "> Note: the `sklearn.preprocessing.SimpleImputer` class also doesn't have a `get_feature_names` method - so we'll work around that below as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "# Inherit sklearn's BaseEstimator and TransformerMixin classes to make these new\n",
    "# classes play nicely with the rest of the `sklearn.compose` functionality we're using\n",
    "class PassthroughEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        self.feature_names = list(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "class SimpleImputerWithFeatureNames(SimpleImputer):\n",
    "    # X is a pandas.DataFrame or pandas.Series, in this case\n",
    "    def fit(self, X, y = None):\n",
    "        self.feature_names = list(X)\n",
    "        # Execute parent class method - the nuts and bolts of this child object\n",
    "        super().fit(X)\n",
    "        return self\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = []\n",
    "for s in strategies:\n",
    "    col, kind = s['col'], s['kind']\n",
    "    \n",
    "    # An opinionated encoding mechanism\n",
    "    if kind == 'categorical':\n",
    "        transformer = OneHotEncoder(sparse=False)\n",
    "    elif kind == 'continuous':\n",
    "        # Default to not applying any preprocessing to continuous features\n",
    "        transformer = PassthroughEncoder()\n",
    "    else:\n",
    "        # Add support at some point for other data types\n",
    "        pass\n",
    "    \n",
    "    result = (col, transformer, [col])\n",
    "    transformers.append(result)\n",
    "\n",
    "trans = ColumnTransformer(transformers)\n",
    "trans.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the feature names for this `ColumnTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, these feature names don't come out nice and clean. With a little work (check out `lambda s: s.split('__')[0]` for originally continuous features and `lambda s: re.sub('__x0', '', s)` for categorical), you'll have more readable names.\n",
    "\n",
    "#### One procedure to rule them all\n",
    "\n",
    "Wouldn't it be nice to simply pass some configuration to some function and get back our fully encoded analytic dataset? Say no more! But first, let's create a more complex and realistic setup, with many more features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [\n",
    "    {\n",
    "        'col': 'age',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'workclass',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'fnlwgt',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'education-num',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'marital-status',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'occupation',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'relationship',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'race',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'sex',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'capital-gain',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': 'median'\n",
    "    },\n",
    "    {\n",
    "        'col': 'capital-loss',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': 'median'\n",
    "    },\n",
    "    {\n",
    "        'col': 'hours-per',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'native-country',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a couple of functions (the former is called by the latter in this particular implementation) that will enable us to create our \"master\" transformer in one fell swoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_handler(\n",
    "    kind,\n",
    "    fill_value,\n",
    "    ignore_fill_value,\n",
    "    handle_unknown):\n",
    "\n",
    "    ''' Generic feature strategy handler '''\n",
    "    if kind == 'categorical':\n",
    "        return OneHotEncoder(sparse=False, handle_unknown=handle_unknown)\n",
    "    elif kind == 'continuous':\n",
    "        if ignore_fill_value:\n",
    "            return PassthroughEncoder()\n",
    "        else:\n",
    "            # These are the four strategies supported by SimpleImputer currently\n",
    "            if fill_value in {'mean', 'median', 'most_frequent', 'constant'}:\n",
    "                params = {'strategy': fill_value, 'fill_value': None}\n",
    "            else:\n",
    "                params = {'strategy': 'constant', 'fill_value': fill_value}\n",
    "            return SimpleImputerWithFeatureNames(**params)\n",
    "    else:\n",
    "        raise ValueError('Kind \"{}\" invalid. Try \"continuous\" or \"categorical\"'.format(kind))\n",
    "        \n",
    "def build_transformer(strategies, ignore_fill_value, handle_unknown):\n",
    "    ''' Take entire `strategies` (see model_config.py) and create master transformer '''\n",
    "    transformers = []\n",
    "    for s in strategies:\n",
    "        col, kind, fill_value = s['col'], s['kind'], s['fill_value']\n",
    "        transformer = strategy_handler(\n",
    "            kind,\n",
    "            fill_value,\n",
    "            ignore_fill_value,\n",
    "            handle_unknown\n",
    "        )\n",
    "        result = (col, transformer, [col])\n",
    "        transformers.append(result)\n",
    "    return ColumnTransformer(transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `build_transformer` to construct our transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39.  0.  0. ...  1.  0.  0.]\n",
      " [50.  0.  0. ...  1.  0.  0.]\n",
      " [38.  0.  0. ...  1.  0.  0.]\n",
      " ...\n",
      " [58.  0.  0. ...  1.  0.  0.]\n",
      " [22.  0.  0. ...  1.  0.  0.]\n",
      " [52.  0.  0. ...  1.  0.  0.]]\n",
      "['age__age', 'workclass__x0_ ?', 'workclass__x0_ Federal-gov', 'workclass__x0_ Local-gov', 'workclass__x0_ Never-worked', 'workclass__x0_ Private', 'workclass__x0_ Self-emp-inc', 'workclass__x0_ Self-emp-not-inc', 'workclass__x0_ State-gov', 'workclass__x0_ Without-pay', 'fnlwgt__fnlwgt', 'education-num__education-num', 'marital-status__x0_ Divorced', 'marital-status__x0_ Married-AF-spouse', 'marital-status__x0_ Married-civ-spouse', 'marital-status__x0_ Married-spouse-absent', 'marital-status__x0_ Never-married', 'marital-status__x0_ Separated', 'marital-status__x0_ Widowed', 'occupation__x0_ ?', 'occupation__x0_ Adm-clerical', 'occupation__x0_ Armed-Forces', 'occupation__x0_ Craft-repair', 'occupation__x0_ Exec-managerial', 'occupation__x0_ Farming-fishing', 'occupation__x0_ Handlers-cleaners', 'occupation__x0_ Machine-op-inspct', 'occupation__x0_ Other-service', 'occupation__x0_ Priv-house-serv', 'occupation__x0_ Prof-specialty', 'occupation__x0_ Protective-serv', 'occupation__x0_ Sales', 'occupation__x0_ Tech-support', 'occupation__x0_ Transport-moving', 'relationship__x0_ Husband', 'relationship__x0_ Not-in-family', 'relationship__x0_ Other-relative', 'relationship__x0_ Own-child', 'relationship__x0_ Unmarried', 'relationship__x0_ Wife', 'race__x0_ Amer-Indian-Eskimo', 'race__x0_ Asian-Pac-Islander', 'race__x0_ Black', 'race__x0_ Other', 'race__x0_ White', 'sex__x0_ Female', 'sex__x0_ Male', 'capital-gain__capital-gain', 'capital-loss__capital-loss', 'hours-per__hours-per', 'native-country__x0_ ?', 'native-country__x0_ Cambodia', 'native-country__x0_ Canada', 'native-country__x0_ China', 'native-country__x0_ Columbia', 'native-country__x0_ Cuba', 'native-country__x0_ Dominican-Republic', 'native-country__x0_ Ecuador', 'native-country__x0_ El-Salvador', 'native-country__x0_ England', 'native-country__x0_ France', 'native-country__x0_ Germany', 'native-country__x0_ Greece', 'native-country__x0_ Guatemala', 'native-country__x0_ Haiti', 'native-country__x0_ Holand-Netherlands', 'native-country__x0_ Honduras', 'native-country__x0_ Hong', 'native-country__x0_ Hungary', 'native-country__x0_ India', 'native-country__x0_ Iran', 'native-country__x0_ Ireland', 'native-country__x0_ Italy', 'native-country__x0_ Jamaica', 'native-country__x0_ Japan', 'native-country__x0_ Laos', 'native-country__x0_ Mexico', 'native-country__x0_ Nicaragua', 'native-country__x0_ Outlying-US(Guam-USVI-etc)', 'native-country__x0_ Peru', 'native-country__x0_ Philippines', 'native-country__x0_ Poland', 'native-country__x0_ Portugal', 'native-country__x0_ Puerto-Rico', 'native-country__x0_ Scotland', 'native-country__x0_ South', 'native-country__x0_ Taiwan', 'native-country__x0_ Thailand', 'native-country__x0_ Trinadad&Tobago', 'native-country__x0_ United-States', 'native-country__x0_ Vietnam', 'native-country__x0_ Yugoslavia']\n"
     ]
    }
   ],
   "source": [
    "trans = build_transformer(\n",
    "    strategies=strategies,\n",
    "    ignore_fill_value=False,\n",
    "    # We'll come back to this\n",
    "    handle_unknown='ignore'\n",
    ")\n",
    "\n",
    "X = trans.fit_transform(df)\n",
    "print(X)\n",
    "print(trans.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would you look at that? In relatively few lines of code, we've taken nearly a dozen features and applied our opinionated transformation logic to create our analytic dataset. Just imagine how useful this could be when dealing with tens or hundreds of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using transformers on unseen data / in production\n",
    "\n",
    "We most certainly will want to use the strategies and transformation logic assigned to a particular `ColumnTransformer` instance to transform new datasets. There are two common use cases here: 1) creating test/validation datasets for model evaluation; 2) using an already-fitted transformer instance to transform new data in \"production\" (whatever that looks like).\n",
    "\n",
    "Using the functions developed above, we can very easily demonstrate how transformers can be used in both the training and testing/consumption of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = build_transformer(\n",
    "    strategies=strategies,\n",
    "    ignore_fill_value=False,\n",
    "    # We came back to this\n",
    "    handle_unknown='ignore'\n",
    ")\n",
    "\n",
    "train_df = df.copy()\n",
    "test_df = df.copy()\n",
    "\n",
    "# Randomly assign some \"new\" values to several records in the unseen dataset, `test_df`\n",
    "rand_idxs = np.random.randint(test_df.shape[0], size=50)\n",
    "test_df.loc[rand_idxs, ['sex', 'native-country']] = 'DEFINITELY A NEVER-BEFORE-SEEN VALUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (32561, 92)\n",
      "Testing dataset shape: (32561, 92)\n"
     ]
    }
   ],
   "source": [
    "train_X = trans.fit_transform(train_df)\n",
    "test_X = trans.transform(test_df)\n",
    "\n",
    "print(f'Training dataset shape: {train_X.shape}')\n",
    "print(f'Testing dataset shape: {test_X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that setting `handle_unknown='ignore'` effectively ignores any new values in all of the columns. Let's see what happens if we set `handle_unknown='error'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories ['DEFINITELY A NEVER-BEFORE-SEEN VALUE'] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-f610bdcf21ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training dataset shape: {train_X.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_transform_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[0;34m(self, X, y, func, fitted)\u001b[0m\n\u001b[1;32m    391\u001b[0m                               _get_column(X, column), y, weight)\n\u001b[1;32m    392\u001b[0m                 for _, trans, column, weight in self._iter(\n\u001b[0;32m--> 393\u001b[0;31m                     fitted=fitted, replace_strings=True))\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"Expected 2D array, got 1D array instead\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m     \u001b[0;31m# if we have a weight for this transformer, multiply output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    589\u001b[0m                                        copy=True)\n\u001b[1;32m    590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform_new\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     msg = (\"Found unknown categories {0} in column {1}\"\n\u001b[1;32m    108\u001b[0m                            \" during transform\".format(diff, i))\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;31m# Set the problematic rows to an acceptable value and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found unknown categories ['DEFINITELY A NEVER-BEFORE-SEEN VALUE'] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "trans = build_transformer(\n",
    "    strategies=strategies,\n",
    "    ignore_fill_value=False,\n",
    "    handle_unknown='error'\n",
    ")\n",
    "\n",
    "train_X = trans.fit_transform(train_df)\n",
    "test_X = trans.transform(test_df)\n",
    "\n",
    "print(f'Training dataset shape: {train_X.shape}')\n",
    "print(f'Testing dataset shape: {test_X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again - totally expected behavior! We've caught a new value in at least one of the features and the current `ColumnTransformer` instance doesn't know how to handle it!\n",
    "\n",
    "> Note: handling unseen values like this is out of the scope of this talk, so I'll leave that for you to ponder!\n",
    "\n",
    "So, to put things more concisely: you could certainly use a transformer with `handle_unknown='ignore'` when moving your model to production, so as to ignore any new values and coerce the unseen dataset to adhere to the original transformer's configuration.\n",
    "\n",
    "If you want to take things a step further, you can use this handy function to iterate through each feature's transformer (of which the master `ColumnTransformer` object is composed) to see the new values and process the error \"silently.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor(df, transformer):\n",
    "    '''\n",
    "    Iterate through each transformer in an\n",
    "    already-fitted ColumnTransformer to check\n",
    "    whether new data raises any Exceptions (intended or otherwise)\n",
    "    '''\n",
    "\n",
    "    if not hasattr(transformer, 'transformers_'):\n",
    "        print('Transformer must be fitted first')\n",
    "    else:\n",
    "        # Select only those transformers that inherit from the BaseEstimator class\n",
    "        transformers = [\n",
    "            (t_inst, t_col)\n",
    "            for _, t_inst, t_col in transformer.transformers_\n",
    "            if isinstance(t_inst, BaseEstimator)\n",
    "        ]\n",
    "\n",
    "        for t_inst, t_col in transformers:\n",
    "            try:\n",
    "                t_inst.transform(df[t_col])\n",
    "            except Exception as e:\n",
    "                e = str(e)\n",
    "                if 'unknown categories' in e:\n",
    "                    print(f'{e} ... found in column \"{t_col}\"')\n",
    "                else:\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found unknown categories ['DEFINITELY A NEVER-BEFORE-SEEN VALUE'] in column 0 during transform ... found in column \"['sex']\"\n",
      "Found unknown categories ['DEFINITELY A NEVER-BEFORE-SEEN VALUE'] in column 0 during transform ... found in column \"['native-country']\"\n"
     ]
    }
   ],
   "source": [
    "# Fit two effectively identical transformers to the same dataset\n",
    "production_trans = build_transformer(strategies, True, 'ignore')\n",
    "monitoring_trans = build_transformer(strategies, True, 'error')\n",
    "\n",
    "production_trans.fit_transform(train_df)\n",
    "monitoring_trans.fit(train_df)\n",
    "\n",
    "# Remember, `test_df` has \"new\" values\n",
    "production_trans.transform(test_df)\n",
    "monitor(test_df, monitoring_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leave-one-out, rule-of-*n*\n",
    "PassthroughTransformer to get at `get_feature_names`\n",
    "Fill values\n",
    "Accepts any sort-of scikit Estimator, with fit and fit_transform stuff"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
