{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The transformative abilities of `sklearn.compose`: a life-saver in disguise?\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/stable/) is undoubtedly one of the most popular libraries for machine learning (ML). From the algorithms provided in its core API to other useful capabilities like feature selection, pipelining, and evaluation, scikit-learn has positioned itself as a must-have on the toolbelt of many data folks. In mid-2018, a new submodule for the core scikit-learn library was initiated: `sklearn.compose`. While still relatively slim, this module, when coupled with existing scikit-learn modules like `sklearn.preprocessing`, can be powerful. The goal of this tutorial is to demonstrate how to implement a configuration-based approach to machine learning dataset creation. Specifically, we'll use the [sklearn.compose](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.compose) and [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) modules.\n",
    "\n",
    "The most [recent stable release of scikit-learn](https://scikit-learn.org/dev/versions.html) is version 0.21.3. `sklearn.compose`, by all accounts, seems to have appeared around version 0.20, so the capabilities presented by this section of scikit-learn are relatively new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What dataset will we be using?\n",
    "\n",
    "The [University of California, Irvine (UCI) Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Adult) contains a treasure-trove of datasets for ML work. I chose the [\"Adult\" dataset](https://archive.ics.uci.edu/ml/datasets/Adult), which tasks the analyst with predicting, based off of a variety of inputs, whether an adult makes more or less than $50k per year. This dataset comes with a mixture of real, categorical, and integer features, which ought to make for a much more \"real-world\" dataset-processing example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First, some housekeeping\n",
    "\n",
    "If you haven't already, run `sh setup.sh` from the base directory to:\n",
    "\n",
    "1) Set up a virtual environment for dependency management\n",
    "\n",
    "2) Start the Jupyter Notebook server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Getting started with the actual exercise\n",
    "\n",
    "First, we'll load the adult dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gathered from the adult.names file and posted here for your convenience\n",
    "cols = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per',\n",
    "    'native-country',\n",
    "    'makes_gt_50k'\n",
    "]\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at some metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (32561, 15)\n",
      "Data sample:\n",
      "   age          workclass  fnlwgt   education  education-num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital-status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per  native-country makes_gt_50k  \n",
      "0          2174             0         40   United-States        <=50K  \n",
      "1             0             0         13   United-States        <=50K  \n",
      "2             0             0         40   United-States        <=50K  \n",
      "3             0             0         40   United-States        <=50K  \n",
      "4             0             0         40            Cuba        <=50K  \n",
      "Data types:\n",
      "age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per          int64\n",
      "native-country    object\n",
      "makes_gt_50k      object\n",
      "dtype: object\n",
      "Number of unique values by field, for non-numeric features:\n",
      "workclass          9\n",
      "education         16\n",
      "marital-status     7\n",
      "occupation        15\n",
      "relationship       6\n",
      "race               5\n",
      "sex                2\n",
      "native-country    42\n",
      "makes_gt_50k       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of dataset: {df.shape}')\n",
    "print(f'Data sample:\\n{df.head()}')\n",
    "print(f'Data types:\\n{df.dtypes}')\n",
    "print(f'Number of unique values by field, for non-numeric features:\\n{df.select_dtypes(include=[\"object\"]).nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is quite a diversity of fields in this dataset. We have a mixture of continuous (`age`, `capital-gain`, `hours-per`, etc.) and categorical (`workclass`, `education`, etc.) features. There is also relatively low cardinality for the categorical features.\n",
    "\n",
    "Now, a logical next step in the process of building a predictive model would be to perform some exploratory data analysis on each of the potential input features. **For the sake of this exercise**, let's assume we've done that and proceed straight to feature-engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Ah, feature engineering - as the old (well, as old as the term \"data scientist\") adage goes, about 80% of your time will be spent pulling together features for whatever model you're building. Now, the vast majority of this time is spent working with stakeholders, thinking about the domain, and trying to come up with the most relevant predictors for whatever predictive task you're after.\n",
    "\n",
    "However, once you've got all of your main features pulled together, oftentimes that's just the first step (albeit a very large one): you'll likely need to preprocess a lot of the fields in order to make your data play nicely with whatever ML algorithm software you're trying to use.\n",
    "\n",
    "For example, most of the algorithms in Python's main ML libraries don't natively support mixed types in input datasets. That is to say, instead of feeding a vector for `sex` like `['male', 'female', 'male', 'female']` as an input feature, we will instead need to encode this field in a numerical fashion. By far the most common approach for encoding categorical vectors is called \"one-hot encoding.\" Below, I'll show a few (of many) examples of how one-hot encoding can be accomplished in Python.\n",
    "\n",
    "> Note: oftentimes, preprocessing will be applied across the entire dataset - not just for categorical features. For the sake of brevity, I'll only demonstrate the one-hot-encoding approach and leave it up to you to incorporate more sophisticated encoding strategies for features of other types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `pandas.get_dummies`\n",
    "\n",
    "The data-manipulation library `pandas` has a function called `get_dummies`, which creates [\"dummy\" variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)), given some input. Here's an example of how we might encode `sex` using `pandas.get_dummies`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original column:\n",
      "0       Male\n",
      "1       Male\n",
      "2       Male\n",
      "3       Male\n",
      "4     Female\n",
      "5     Female\n",
      "6     Female\n",
      "7       Male\n",
      "8     Female\n",
      "9       Male\n",
      "Name: sex, dtype: object\n",
      "That same column, one-hot-encoded:\n",
      "   sex_ Female  sex_ Male\n",
      "0            0          1\n",
      "1            0          1\n",
      "2            0          1\n",
      "3            0          1\n",
      "4            1          0\n",
      "5            1          0\n",
      "6            1          0\n",
      "7            0          1\n",
      "8            1          0\n",
      "9            0          1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original column:\\n{df['sex'].head(10)}\")\n",
    "print(f\"That same column, one-hot-encoded:\\n{pd.get_dummies(df['sex'], prefix='sex').head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach would be to use [`sklearn.preprocessing.OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That same column, one-hot-encoded:\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Note: when using models prone to perfect collinearity, you'll want to set `drop=True`\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "print(f\"That same column, one-hot-encoded:\\n{enc.fit_transform(df['sex'].values.reshape(-1, 1))[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, both of these approaches are perfectly fine ways of performing one-hot encoding. However, the latter approach will play very nicely with the rest of the `sklearn.compose` module, which I'm here to demonstrate. Technically, `pd.get_dummies` could work as well, but it would take a bit more work, and the main benefit of the second approach is staying within the `scikit-learn` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how does `sklearn.compose` help with all of this preprocessing?\n",
    "\n",
    "If you look at the [main page for `sklearn.compose`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.compose), you'll notice how few functions/classes exist within that submodule. We're most concerned with `ColumnTransformer` and `make_column_transformer`. From the `ColumnTransformer` description:\n",
    "\n",
    "> Applies transformers to columns of an array or pandas DataFrame.\n",
    "This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.\n",
    "\n",
    "The `make_column_transformer` is simply shorthand for `ColumnTransformer`, and doesn't support as many options as its namesake, so for this exercise we'll concern ourselves primarily with the latter.\n",
    "\n",
    "Effectively, the code provided through the `compose` submodule will allow us to very easily construct analytic, ready-to-be-modeled-off-of datasets, using pre-defined encoding patterns.\n",
    "\n",
    "### Enough talk â€“ how does a `ColumnTransformer` work?\n",
    "\n",
    "Let's go through the one-hot encoding example from above, using a `ColumnTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "col = 'sex'\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "trans = ColumnTransformer([(col, enc, [col])])\n",
    "trans.fit_transform(df)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we've successfully used a `ColumnTransformer` and the typical scikit-learn `fit` and `transform` patterns to one-hot-encode the `sex` column, just like we did above.\n",
    "\n",
    "<img src=\"imgs/ytho.jpg\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of the `ColumnTransformer` is seen when you're dealing with trying to abstract more complex encoding pipelines across a \"wide\" dataset. Sure, properly encoding `sex` by itself is a trivial task. But what if you have to apply *d*-number of encoding strategies across *n*-number of columns, and want a consistent and \"summarized\" way of doing so? Allow me to demonstrate.\n",
    "\n",
    "We're effectively going to treat the encoding/preprocessing step as a configuration file problem. First, we'll select a few columns (of whatever type), and specify what type of feature the column represents.\n",
    "\n",
    "> Note: you could also get at the type of feature by relying on pandas' default data-type parsing when the file is initially read, i.e. looking at `df.dtypes`. What is shown in this tutorial is a more explicit approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0., ...,  0.,  1., 39.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 50.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 38.],\n",
       "       ...,\n",
       "       [ 1.,  0.,  0., ...,  0.,  1., 58.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1., 22.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  1., 52.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategies = [\n",
    "    {\n",
    "        'col': 'sex',\n",
    "        'kind': 'categorical'\n",
    "    },\n",
    "    {\n",
    "        'col':  'race',\n",
    "        'kind': 'categorical'\n",
    "    },\n",
    "    {\n",
    "        'col': 'age',\n",
    "        'kind': 'continuous'\n",
    "    }\n",
    "]\n",
    "\n",
    "transformers = []\n",
    "for s in strategies:\n",
    "    col, kind = s['col'], s['kind']\n",
    "    \n",
    "    # An opinionated encoding mechanism\n",
    "    if kind == 'categorical':\n",
    "        transformer = OneHotEncoder(sparse=False)\n",
    "    elif kind == 'continuous':\n",
    "        # Default to not applying any preprocessing to continuous features\n",
    "        transformer = 'passthrough'\n",
    "    else:\n",
    "        # Add support at some point for other data types\n",
    "        pass\n",
    "    \n",
    "    result = (col, transformer, [col])\n",
    "    transformers.append(result)\n",
    "\n",
    "master_trans = ColumnTransformer(transformers)\n",
    "master_trans.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional topics\n",
    "\n",
    "In the example above, we went from a raw, unprocessed dataset to something ready for further analysis/modeling, all in very few lines of code thanks to our dataset-as-a-configuration-file approach.\n",
    "\n",
    "But there is still more to touch on here. Below I'll detail some additional topics related to `sklearn.compose`.\n",
    "\n",
    "#### Getting names of encoded features\n",
    "\n",
    "All we see above is a NumPy matrix containing a bunch of numbers. What if we wanted to more easily inspect or share this dataset? One of the first things we may want to know is the human-readable name for each column.\n",
    "\n",
    "In comes the `get_feature_names` method off of our `ColumnTransformer` object. Let's see what results we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "get_feature_names is not yet supported when using a 'passthrough' transformer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d91e9be7d35f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaster_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 raise NotImplementedError(\n\u001b[0;32m--> 334\u001b[0;31m                     \u001b[0;34m\"get_feature_names is not yet supported when using \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                     \"a 'passthrough' transformer.\")\n\u001b[1;32m    336\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_feature_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: get_feature_names is not yet supported when using a 'passthrough' transformer."
     ]
    }
   ],
   "source": [
    "master_trans.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error is to be expected (at least at this point in `sklearn.compose`'s development cycle)! Since we've used the `'passthrough'` option for some of our features, we don't have the ability to get the name of the feature than was originally transformed. But, just because that capability isn't yet enabled for the \"passthrough\" transformation doesn't mean we can't write that functionality ourselves! ðŸ˜ˆ\n",
    "\n",
    "> Note: the `sklearn.preprocessing.SimpleImputer` class also doesn't have a `get_feature_names` method - so we'll work around that below as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "# Inherit sklearn's BaseEstimator and TransformerMixin classes to make these new\n",
    "# classes play nicely with the rest of the `sklearn.compose` functionality we're using\n",
    "class PassthroughEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        self.feature_names = list(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "class SimpleImputerWithFeatureNames(SimpleImputer):\n",
    "    # X is a pandas.DataFrame or pandas.Series, in this case\n",
    "    def fit(self, X, y = None):\n",
    "        self.feature_names = list(X)\n",
    "        # Execute parent class method - the nuts and bolts of this child object\n",
    "        super().fit(X)\n",
    "        return self\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = []\n",
    "for s in strategies:\n",
    "    col, kind = s['col'], s['kind']\n",
    "    \n",
    "    # An opinionated encoding mechanism\n",
    "    if kind == 'categorical':\n",
    "        # Note that OneHotEncoder already has a `get_feature_names` method\n",
    "        transformer = OneHotEncoder(sparse=False)\n",
    "    elif kind == 'continuous':\n",
    "        # Default to not applying any preprocessing to continuous features\n",
    "        transformer = PassthroughEncoder()\n",
    "    else:\n",
    "        # Add support at some point for other data types\n",
    "        pass\n",
    "    \n",
    "    result = (col, transformer, [col])\n",
    "    transformers.append(result)\n",
    "\n",
    "trans = ColumnTransformer(transformers)\n",
    "trans.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the feature names for this `ColumnTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, these feature names don't come out nice and clean. With a little work (check out `lambda s: s.split('__')[0]` for originally continuous features and `lambda s: re.sub('__x0', '', s)` for categorical), you'll have more readable names.\n",
    "\n",
    "#### One procedure to rule them all\n",
    "\n",
    "Wouldn't it be nice to simply pass some configuration to some function and get back our fully encoded analytic dataset? Say no more! But first, let's create a more complex and realistic setup, with many more features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [\n",
    "    {\n",
    "        'col': 'age',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'workclass',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'fnlwgt',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'education-num',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'marital-status',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'occupation',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'relationship',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'race',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'sex',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'capital-gain',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': 'median'\n",
    "    },\n",
    "    {\n",
    "        'col': 'capital-loss',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': 'median'\n",
    "    },\n",
    "    {\n",
    "        'col': 'hours-per',\n",
    "        'kind': 'continuous',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "    {\n",
    "        'col': 'native-country',\n",
    "        'kind': 'categorical',\n",
    "        'fill_value': np.nan\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a couple of functions (the former is called by the latter in this particular implementation) that will enable us to create our \"master\" transformer in one fell swoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_handler(\n",
    "    kind,\n",
    "    fill_value,\n",
    "    ignore_fill_value,\n",
    "    handle_unknown):\n",
    "\n",
    "    ''' Generic feature strategy handler '''\n",
    "    if kind == 'categorical':\n",
    "        return OneHotEncoder(sparse=False, handle_unknown=handle_unknown)\n",
    "    elif kind == 'continuous':\n",
    "        if ignore_fill_value:\n",
    "            return PassthroughEncoder()\n",
    "        else:\n",
    "            # These are the four strategies supported by SimpleImputer currently\n",
    "            if fill_value in {'mean', 'median', 'most_frequent', 'constant'}:\n",
    "                params = {'strategy': fill_value, 'fill_value': None}\n",
    "            else:\n",
    "                params = {'strategy': 'constant', 'fill_value': fill_value}\n",
    "            return SimpleImputerWithFeatureNames(**params)\n",
    "    else:\n",
    "        raise ValueError(f'Kind \"{kind}\" invalid. Try \"continuous\" or \"categorical\"')\n",
    "\n",
    "def build_transformer(strategies, ignore_fill_value, handle_unknown):\n",
    "    ''' Take entire `strategies` (see model_config.py) and create master transformer '''\n",
    "    transformers = []\n",
    "    for s in strategies:\n",
    "        col, kind, fill_value = s['col'], s['kind'], s['fill_value']\n",
    "        transformer = strategy_handler(\n",
    "            kind,\n",
    "            fill_value,\n",
    "            ignore_fill_value,\n",
    "            handle_unknown\n",
    "        )\n",
    "        result = (col, transformer, [col])\n",
    "        transformers.append(result)\n",
    "    return ColumnTransformer(transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `build_transformer` to construct our transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = build_transformer(\n",
    "    strategies=strategies,\n",
    "    ignore_fill_value=False,\n",
    "    # We'll come back to this\n",
    "    handle_unknown='ignore'\n",
    ")\n",
    "\n",
    "X = trans.fit_transform(df)\n",
    "print(X)\n",
    "print(trans.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would you look at that? In relatively few lines of code, we've taken nearly a dozen features and applied our opinionated transformation logic to create our analytic dataset. Just imagine how useful this could be when dealing with tens or hundreds of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using transformers on unseen data / in production\n",
    "\n",
    "We most certainly will want to use the strategies and transformation logic assigned to a particular `ColumnTransformer` instance to transform new datasets. There are two common use cases here: 1) creating test/validation datasets for model evaluation; 2) using an already-fitted transformer instance to transform new data in \"production\" (whatever that looks like).\n",
    "\n",
    "Using the functions developed above, we can very easily demonstrate how transformers can be used in both the training and testing/consumption of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = build_transformer(\n",
    "    strategies=strategies,\n",
    "    ignore_fill_value=False,\n",
    "    # We came back to this\n",
    "    handle_unknown='ignore'\n",
    ")\n",
    "\n",
    "train_df = df.copy()\n",
    "test_df = df.copy()\n",
    "\n",
    "# Randomly assign some \"new\" values to several records in the unseen dataset, `test_df`\n",
    "rand_idxs = np.random.randint(test_df.shape[0], size=50)\n",
    "test_df.loc[rand_idxs, ['sex', 'native-country']] = 'DEFINITELY A NEVER-BEFORE-SEEN VALUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = trans.fit_transform(train_df)\n",
    "test_X = trans.transform(test_df)\n",
    "\n",
    "print(f'Training dataset shape: {train_X.shape}')\n",
    "print(f'Testing dataset shape: {test_X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that setting `handle_unknown='ignore'` effectively ignores any new values in all of the columns. Let's see what happens if we set `handle_unknown='error'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = build_transformer(\n",
    "    strategies=strategies,\n",
    "    ignore_fill_value=False,\n",
    "    handle_unknown='error'\n",
    ")\n",
    "\n",
    "train_X = trans.fit_transform(train_df)\n",
    "test_X = trans.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again - totally expected behavior! We've caught a new value in at least one of the features and the current `ColumnTransformer` instance doesn't know how to handle it!\n",
    "\n",
    "> Note: handling unseen values like this is out of the scope of this talk, so I'll leave that for you to ponder!\n",
    "\n",
    "So, to put things more concisely: you could certainly use a transformer with `handle_unknown='ignore'` when moving your model to production, so as to ignore any new values and coerce the unseen dataset to adhere to the original transformer's configuration.\n",
    "\n",
    "If you want to take things a step further, you can use this handy function to iterate through each feature's transformer (of which the master `ColumnTransformer` object is composed) to see the new values and process the error \"silently.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor(df, transformer):\n",
    "    '''\n",
    "    Iterate through each transformer in an\n",
    "    already-fitted ColumnTransformer to check\n",
    "    whether new data raises any Exceptions (intended or otherwise)\n",
    "    '''\n",
    "\n",
    "    if not hasattr(transformer, 'transformers_'):\n",
    "        print('Transformer must be fitted first')\n",
    "    else:\n",
    "        # Select only those transformers that inherit from the BaseEstimator class\n",
    "        transformers = [\n",
    "            (t_inst, t_col)\n",
    "            for _, t_inst, t_col in transformer.transformers_\n",
    "            if isinstance(t_inst, BaseEstimator)\n",
    "        ]\n",
    "\n",
    "        for t_inst, t_col in transformers:\n",
    "            try:\n",
    "                t_inst.transform(df[t_col])\n",
    "            except Exception as e:\n",
    "                e = str(e)\n",
    "                if 'unknown categories' in e:\n",
    "                    print(f'{e} ... found in column \"{t_col}\"')\n",
    "                else:\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit two effectively identical transformers to the same dataset\n",
    "production_trans = build_transformer(strategies, True, 'ignore')\n",
    "monitoring_trans = build_transformer(strategies, True, 'error')\n",
    "\n",
    "production_trans.fit_transform(train_df)\n",
    "monitoring_trans.fit(train_df)\n",
    "\n",
    "# Remember, `test_df` has \"new\" values\n",
    "production_trans.transform(test_df)\n",
    "monitor(test_df, monitoring_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "\n",
    "### What about the leave-one-out procedure for avoiding perfect [collinearity](https://en.wikipedia.org/wiki/Multicollinearity)?\n",
    "\n",
    "Currently, this pipeline doesn't accommodate \"leave-one-out\" functionality, as is required for typical linear models like logistic and linear regression. I'm fairly sure, with a few small tweaks to `OneHotEncoder`, this could be accomplished - I haven't had the time to investigate, and I almost exclusively use these transformers with tree-based models, which don't succumb to collinearity concerns.\n",
    "\n",
    "### What about the [\"rule-of-*n*\"](https://whatis.techtarget.com/definition/rule-of-five-statistics)?\n",
    "\n",
    "In its current state, this transformation workflow also doesn't account for the total number of occurrences of a particular value for a particular feature. Oftentimes, depending on your task, you might choose to remove values for features with little [support](https://en.wikipedia.org/wiki/Association_rule_learning#Support), because few cases having that condition typically are not enough to draw valid inference. Again, most tree-based model implementation (e.g. those in `sklearn` and `xgboost`) will skirt this issue by limiting the number of instances required to occur at a leaf node. Features with support less than that limit will not be chosen for splits in these kinds of models."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
